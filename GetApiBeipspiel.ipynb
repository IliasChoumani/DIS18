{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schritte:\n",
    "1. Urls/IDS herausfinden em die Apis abzurufen\n",
    "1. Api der Website abrufen (mit python on einem Jupyter Notebook) brauchen request und/oder Beautiful Soup der website parsen\n",
    "2. Spezifizierung welche Daten abgerufen werden(welche personenbezogenen daten, wer hat bei wem promoviert, welche artikel wurden von der Person pupliziert)\n",
    "3. wie werden daten gespeichert (daten in xml beschaffen und in json umwandeln), welche formate sind mit wikidata kompatibel(am besten json)\n",
    "4. Daten bereiinigen/cleanen (fehlende oder fehlerhafte daten entfernen)\n",
    "5. wikidata einträge der autoren automatisiert findbar machen (ids beschaffen)\n",
    "6. Wikidata daten herunterladen\n",
    "7. datenabgleich unsere daten/Wikidata daten (Wikidata json export service)\n",
    "8. löschen von dublikaten \n",
    "9. automatisiertes hochladen der daten welche in wikidata nicht existieren (über Api zugriff) welche tool kann man dafür verwenden? (wikitools bibliothek?), muss man vorher schreibzugriff beatragen?\n",
    "\n",
    "Datensatz mit welchem wir ein beispiel durcharbeiten (Professor Dr. Alexander Goesmann versuchen laufende Projekte, welche in gepris stehen in Wikidata einzufügen https://gepris.dfg.de/gepris/person/188428736)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Beispiel mit einer Person \n",
    "https://gepris.dfg.de/gepris/person/188428736"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (4.9.3)"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 24.0 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\n",
      "\n",
      "[notice] A new release of pip is available: 23.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Requirement already satisfied: requests in c:\\users\\ilias\\anaconda3\\lib\\site-packages (2.24.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.0.1)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (from requests) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (from requests) (2.10)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (from requests) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\ilias\\anaconda3\\lib\\site-packages (from requests) (2020.6.20)\n"
     ]
    }
   ],
   "source": [
    "# Installiere beautifulsoup4 und requests\n",
    "!pip install beautifulsoup4 requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFG - GEPRIS - Professor Dr. Alexander  Goesmann\n",
      "None\n",
      "#inhalt\n",
      "#toolbar\n",
      "http://www.dfg.de\n",
      "/gepris/OCTOPUS?task=showContact\n",
      "/gepris/OCTOPUS?task=showSearchHelp\n",
      "/gepris/OCTOPUS?task=showMonitor\n",
      "/gepris/person/188428736?language=en\n",
      "/gepris\n",
      "/gepris/OCTOPUS?task=showSearchSimple\n",
      "/gepris/OCTOPUS?task=showKatalog\n",
      "/gepris/OCTOPUS?task=browsePersonIndex\n",
      "/gepris/OCTOPUS?task=browseOrtsindex\n",
      "/gepris/OCTOPUS?task=showAbout\n",
      "None\n",
      "/gepris/OCTOPUS\n",
      "/gepris/person/188428736?displayMode=print\n",
      "\n",
      "None\n",
      "None\n",
      "/gepris/projekt/319835486\n",
      "/gepris/projekt/447603908\n",
      "/gepris/projekt/456668568\n",
      "/gepris/projekt/458957343\n",
      "None\n",
      "None\n",
      "/gepris/projekt/325443116\n",
      "None\n",
      "/gepris/projekt/221270173\n",
      "None\n",
      "None\n",
      "/gepris/projekt/284237345\n",
      "None\n",
      "/gepris/projekt/183605059\n",
      "None\n",
      "None\n",
      "/gepris/projekt/442032008\n",
      "/gepris/projekt/460129525\n",
      "None\n",
      "None\n",
      "/gepris/projekt/255821879\n",
      "None\n",
      "None\n",
      "/gepris/projekt/507302435\n",
      "None\n",
      "/gepris/projekt/270041755\n",
      "None\n",
      "None\n",
      "/gepris/projekt/491261247\n",
      "/gepris/OCTOPUS?task=showContact\n",
      "/gepris/OCTOPUS?task=showPrivacyPolicy\n",
      "None\n",
      "/gepris/person/188428736?fontSize=0\n",
      "/gepris/person/188428736?fontSize=1\n",
      "/gepris/person/188428736?fontSize=2\n",
      "/gepris/person/188428736?contrast=0\n",
      "/gepris/person/188428736?contrast=1\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Jetzt kannst du auf die Elemente der Seite zugreifen und sie durchsuchen\n",
    "    # Beispiel: Drucke den Titel der Seite\n",
    "    print(soup.title.text)\n",
    "\n",
    "    # Beispiel: Drucke alle Links auf der Seite\n",
    "    for link in soup.find_all('a'):\n",
    "        print(link.get('href'))\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/gepris/projekt/319835486\n",
      "/gepris/projekt/447603908\n",
      "/gepris/projekt/456668568\n",
      "/gepris/projekt/458957343\n",
      "/gepris/projekt/325443116\n",
      "/gepris/projekt/221270173\n",
      "/gepris/projekt/284237345\n",
      "/gepris/projekt/183605059\n",
      "/gepris/projekt/442032008\n",
      "/gepris/projekt/460129525\n",
      "/gepris/projekt/255821879\n",
      "/gepris/projekt/507302435\n",
      "/gepris/projekt/270041755\n",
      "/gepris/projekt/491261247\n"
     ]
    }
   ],
   "source": [
    "# gibt alle projekte an denen die person arbeitet wieder\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Finde alle Links auf der Seite\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filtere nur die Links, die auf Projekte verweisen\n",
    "    project_links = [link['href'] for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "    # Drucke die gefundenen Projekt-Links\n",
    "    for project_link in project_links:\n",
    "        print(project_link)\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DFG - GEPRIS - Professor Dr. Konrad  Förstner\n",
      "None\n",
      "#inhalt\n",
      "#toolbar\n",
      "http://www.dfg.de\n",
      "/gepris/OCTOPUS?task=showContact\n",
      "/gepris/OCTOPUS?task=showSearchHelp\n",
      "/gepris/OCTOPUS?task=showMonitor\n",
      "/gepris/person/262159783?language=en\n",
      "/gepris\n",
      "/gepris/OCTOPUS?task=showSearchSimple\n",
      "/gepris/OCTOPUS?task=showKatalog\n",
      "/gepris/OCTOPUS?task=browsePersonIndex\n",
      "/gepris/OCTOPUS?task=browseOrtsindex\n",
      "/gepris/OCTOPUS?task=showAbout\n",
      "None\n",
      "/gepris/OCTOPUS\n",
      "/gepris/person/262159783?displayMode=print\n",
      "\n",
      "None\n",
      "None\n",
      "/gepris/projekt/433110396\n",
      "/gepris/projekt/492813820\n",
      "/gepris/projekt/509313233\n",
      "None\n",
      "None\n",
      "/gepris/projekt/460129525\n",
      "/gepris/projekt/521476232\n",
      "/gepris/OCTOPUS?task=showContact\n",
      "/gepris/OCTOPUS?task=showPrivacyPolicy\n",
      "None\n",
      "/gepris/person/262159783?fontSize=0\n",
      "/gepris/person/262159783?fontSize=1\n",
      "/gepris/person/262159783?fontSize=2\n",
      "/gepris/person/262159783?contrast=0\n",
      "/gepris/person/262159783?contrast=1\n"
     ]
    }
   ],
   "source": [
    "# beispiel mit förstner zum vergleichen\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/262159783'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Jetzt kannst du auf die Elemente der Seite zugreifen und sie durchsuchen\n",
    "    # Beispiel: Drucke den Titel der Seite\n",
    "    print(soup.title.text)\n",
    "\n",
    "    # Beispiel: Drucke alle Links auf der Seite\n",
    "    for link in soup.find_all('a'):\n",
    "        print(link.get('href'))\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Core Unit - Genom Signaturen und integrierte Systembiologie der Pathogen-Wirts Interaktion\n",
      "Bioinformatische Methoden zur Analyse der mutualistischen RNA-Kommunikation\n",
      "Bioinformatische Workflows zur skalierbaren Analyse von pflanzlichen “Omics”-Daten in Cloud-Computing-Umgebungen\n",
      "Evolution von Gennetzwerken: Die Ranunculales als Modellordnung für evolutionäre Innovationen\n",
      "GRK 2355: Regulatorische Netzwerke im mRNA-Lebenszyklus:\n",
      "von kodierenden zu nichtkodierenden RNAs\n",
      "GRK 1906: Informatische Methoden für die Analyse von Genomdiversität und -dynamik\n",
      "KFO 309: Virus-induced Lung Injury:\n",
      "Pathobiology and Novel Therapeutic Strategies\n",
      "Bioinformatische Analyse genomweiter Datensätze\n",
      "NFDI4BioDiversität – Biodiversität, Ökologie und Umweltdaten\n",
      "NFDI4Microbiota - Nationale Forschungsdateninfrastruktur für Mikrobiota-Forschung\n",
      "Genom-weite Untersuchungen zur Artbildung unter sympatrischen Bedingungen in einem marinen Säuger: demographische Entwicklungsgeschichte, Populationsstruktur und lokale Anpassung im vom Aussterben bedrohten Galapagos Seelöwen\n",
      "Die Rolle von SOCS1 Mutationen bei der Pathogenese von Hodgkin Lymphomen und Diffus großzelligen B-Zell Lymphomen\n",
      "Nukleosom-Präservierung in Säugetier-Spermien: ein epigenetisches Programm zur Wahrung der gesunden männlichen Reproduktion\n",
      "Untersuchung der Mechanismen für die Stabilität von mcr-1-kodierenden IncX4-Mechanismen\n"
     ]
    }
   ],
   "source": [
    "#namen der projekte von Professor Dr. Alexander Goesmann\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Finde alle Links auf der Seite\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filtere nur die Links, die auf Projekte verweisen\n",
    "    project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "    # Extrahiere den Text (Namen) aus den gefundenen Projekt-Links\n",
    "    project_names = [project_link.get_text(strip=True) for project_link in project_links]\n",
    "\n",
    "    # Drucke die gefundenen Projektnamen\n",
    "    for project_name in project_names:\n",
    "        print(project_name)\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Projekt Name: Core Unit - Genom Signaturen und integrierte Systembiologie der Pathogen-Wirts Interaktion\n",
      "Link: /gepris/projekt/319835486\n",
      "\n",
      "Projekt Name: Bioinformatische Methoden zur Analyse der mutualistischen RNA-Kommunikation\n",
      "Link: /gepris/projekt/447603908\n",
      "\n",
      "Projekt Name: Bioinformatische Workflows zur skalierbaren Analyse von pflanzlichen “Omics”-Daten in Cloud-Computing-Umgebungen\n",
      "Link: /gepris/projekt/456668568\n",
      "\n",
      "Projekt Name: Evolution von Gennetzwerken: Die Ranunculales als Modellordnung für evolutionäre Innovationen\n",
      "Link: /gepris/projekt/458957343\n",
      "\n",
      "Projekt Name: GRK 2355: Regulatorische Netzwerke im mRNA-Lebenszyklus:\n",
      "von kodierenden zu nichtkodierenden RNAs\n",
      "Link: /gepris/projekt/325443116\n",
      "\n",
      "Projekt Name: GRK 1906: Informatische Methoden für die Analyse von Genomdiversität und -dynamik\n",
      "Link: /gepris/projekt/221270173\n",
      "\n",
      "Projekt Name: KFO 309: Virus-induced Lung Injury:\n",
      "Pathobiology and Novel Therapeutic Strategies\n",
      "Link: /gepris/projekt/284237345\n",
      "\n",
      "Projekt Name: Bioinformatische Analyse genomweiter Datensätze\n",
      "Link: /gepris/projekt/183605059\n",
      "\n",
      "Projekt Name: NFDI4BioDiversität – Biodiversität, Ökologie und Umweltdaten\n",
      "Link: /gepris/projekt/442032008\n",
      "\n",
      "Projekt Name: NFDI4Microbiota - Nationale Forschungsdateninfrastruktur für Mikrobiota-Forschung\n",
      "Link: /gepris/projekt/460129525\n",
      "\n",
      "Projekt Name: Genom-weite Untersuchungen zur Artbildung unter sympatrischen Bedingungen in einem marinen Säuger: demographische Entwicklungsgeschichte, Populationsstruktur und lokale Anpassung im vom Aussterben bedrohten Galapagos Seelöwen\n",
      "Link: /gepris/projekt/255821879\n",
      "\n",
      "Projekt Name: Die Rolle von SOCS1 Mutationen bei der Pathogenese von Hodgkin Lymphomen und Diffus großzelligen B-Zell Lymphomen\n",
      "Link: /gepris/projekt/507302435\n",
      "\n",
      "Projekt Name: Nukleosom-Präservierung in Säugetier-Spermien: ein epigenetisches Programm zur Wahrung der gesunden männlichen Reproduktion\n",
      "Link: /gepris/projekt/270041755\n",
      "\n",
      "Projekt Name: Untersuchung der Mechanismen für die Stabilität von mcr-1-kodierenden IncX4-Mechanismen\n",
      "Link: /gepris/projekt/491261247\n",
      "\n",
      "Name der Person: DFG - GEPRIS - Professor Dr. Alexander  Goesmann\n"
     ]
    }
   ],
   "source": [
    "#projekt link und name des Projektes und name der person \n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "    person_name = soup.title.text.strip()\n",
    "\n",
    "    # Finde alle Links auf der Seite\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filtere nur die Links, die auf Projekte verweisen\n",
    "    project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "    # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "    for project_link in project_links:\n",
    "        project_name = project_link.get_text(strip=True)\n",
    "        project_url = project_link['href']\n",
    "        print(f\"Projekt Name: {project_name}\\nLink: {project_url}\\n\")\n",
    "\n",
    "    # Drucke den Namen der Person\n",
    "    print(f\"Name der Person: {person_name}\")\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### erstellen einer liste aller Ids der 10 Personen und ausgabe derer projekte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# kein eintrag haben: jochen blom, Marius Dieckmann, Barbara Götz, Thomas Gübitz, Franziska Hufsky\n",
    "liste_aller_personen = [\"https://gepris.dfg.de/gepris/person/262159783\",\n",
    "                       \"https://gepris.dfg.de/gepris/person/1592887\",\n",
    "                       \"https://gepris.dfg.de/gepris/person/1011382\",\n",
    "                       \"https://gepris.dfg.de/gepris/person/184220620\",\n",
    "                        \"https://gepris.dfg.de/gepris/person/188428736\",       \n",
    "                       ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich in 'output.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Initialisiere eine leere Liste für die Projekte\n",
    "projects_data = []\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "    person_name = soup.title.text.strip()\n",
    "\n",
    "    # Finde alle Links auf der Seite\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filtere nur die Links, die auf Projekte verweisen\n",
    "    project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "    # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "    for project_link in project_links:\n",
    "        project_name = project_link.get_text(strip=True)\n",
    "        project_url = project_link['href']\n",
    "        projects_data.append({\n",
    "            \"Person Name\": person_name,\n",
    "            \"Projekt Name\": project_name,\n",
    "            \"Projekt Link\": project_url\n",
    "        })\n",
    "\n",
    "    # Speichere die Daten in einer JSON-Datei\n",
    "    with open('output.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(projects_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Drucke eine Bestätigungsmeldung\n",
    "    print(\"Daten erfolgreich in 'output.json' gespeichert.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten erfolgreich in 'output.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# URL der Website\n",
    "url = 'https://gepris.dfg.de/gepris/person/188428736'\n",
    "\n",
    "# Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "response = requests.get(url)\n",
    "\n",
    "# Initialisiere ein leeres Dictionary für die Person und ihre Projekte\n",
    "person_data = {}\n",
    "\n",
    "# Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "if response.status_code == 200:\n",
    "    # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "    person_name = soup.title.text.strip()\n",
    "    person_data[\"Person Name\"] = person_name\n",
    "\n",
    "    # Finde alle Links auf der Seite\n",
    "    links = soup.find_all('a', href=True)\n",
    "\n",
    "    # Filtere nur die Links, die auf Projekte verweisen\n",
    "    project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "    # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "    projects_data = []\n",
    "    for project_link in project_links:\n",
    "        project_name = project_link.get_text(strip=True)\n",
    "        project_url = project_link['href']\n",
    "        projects_data.append({\n",
    "            \"Projekt Name\": project_name,\n",
    "            \"Projekt Link\": project_url\n",
    "        })\n",
    "\n",
    "    # Füge die Projekte zum Dictionary der Person hinzu\n",
    "    person_data[\"Projekte\"] = projects_data\n",
    "\n",
    "    # Speichere die Daten in einer JSON-Datei\n",
    "    with open('output.json', 'w', encoding='utf-8') as json_file:\n",
    "        json.dump(person_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "    # Drucke eine Bestätigungsmeldung\n",
    "    print(\"Daten erfolgreich in 'output.json' gespeichert.\")\n",
    "\n",
    "else:\n",
    "    print(f\"Fehler bei der Anfrage: {response.status_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Daten erfolgreich in 'output_all_persons.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Liste aller Personen-URLs\n",
    "liste_aller_personen = [\n",
    "    \"https://gepris.dfg.de/gepris/person/262159783\",\n",
    "    \"https://gepris.dfg.de/gepris/person/1592887\",\n",
    "    \"https://gepris.dfg.de/gepris/person/1011382\",\n",
    "    \"https://gepris.dfg.de/gepris/person/184220620\",\n",
    "    \"https://gepris.dfg.de/gepris/person/188428736\",\n",
    "]\n",
    "\n",
    "# Initialisiere eine leere Liste für alle Personen und ihre Projekte\n",
    "all_persons_data = []\n",
    "\n",
    "# Durchlaufe die Liste aller Personen\n",
    "for url in liste_aller_personen:\n",
    "    # Initialisiere ein leeres Dictionary für die Person und ihre Projekte\n",
    "    person_data = {}\n",
    "\n",
    "    # Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "        person_name = soup.title.text.strip()\n",
    "        person_data[\"Person Name\"] = person_name\n",
    "\n",
    "        # Finde alle Links auf der Seite\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtere nur die Links, die auf Projekte verweisen\n",
    "        project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "        # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "        projects_data = []\n",
    "        for project_link in project_links:\n",
    "            project_name = project_link.get_text(strip=True)\n",
    "            project_url = project_link['href']\n",
    "            projects_data.append({\n",
    "                \"Projekt Name\": project_name,\n",
    "                \"Projekt Link\": project_url\n",
    "            })\n",
    "\n",
    "        # Füge die Projekte zum Dictionary der Person hinzu\n",
    "        person_data[\"Projekte\"] = projects_data\n",
    "\n",
    "        # Füge die Person zum Gesamtdictionary hinzu\n",
    "        all_persons_data.append(person_data)\n",
    "\n",
    "    else:\n",
    "        print(f\"Fehler bei der Anfrage für {url}: {response.status_code}\")\n",
    "\n",
    "# Speichere alle Daten in einer JSON-Datei\n",
    "with open('output_all_persons.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(all_persons_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Drucke eine Bestätigungsmeldung\n",
    "print(\"Alle Daten erfolgreich in 'output_all_persons.json' gespeichert.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Daten erfolgreich in 'output_all_persons.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "\n",
    "# Liste aller Personen-URLs\n",
    "liste_aller_personen = [\n",
    "    \"https://gepris.dfg.de/gepris/person/262159783\",\n",
    "    \"https://gepris.dfg.de/gepris/person/1592887\",\n",
    "    \"https://gepris.dfg.de/gepris/person/1011382\",\n",
    "    \"https://gepris.dfg.de/gepris/person/184220620\",\n",
    "    \"https://gepris.dfg.de/gepris/person/188428736\",\n",
    "]\n",
    "\n",
    "# Initialisiere eine leere Liste für alle Personen und ihre Projekte\n",
    "all_persons_data = []\n",
    "\n",
    "# Durchlaufe die Liste aller Personen\n",
    "for url in liste_aller_personen:\n",
    "    # Initialisiere ein leeres Dictionary für die Person und ihre Projekte\n",
    "    person_data = {}\n",
    "\n",
    "    # Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "        person_name = soup.title.text.strip().replace(\"DFG - GEPRIS - \", \"\")\n",
    "        person_data[\"Person Name\"] = person_name\n",
    "\n",
    "        # Finde alle Links auf der Seite\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtere nur die Links, die auf Projekte verweisen\n",
    "        project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "        # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "        projects_data = []\n",
    "        for project_link in project_links:\n",
    "            project_name = project_link.get_text(strip=True)\n",
    "            project_url = project_link['href']\n",
    "            projects_data.append({\n",
    "                \"Projekt Name\": project_name,\n",
    "                \"Projekt Link\": project_url\n",
    "            })\n",
    "\n",
    "        # Füge die Projekte zum Dictionary der Person hinzu\n",
    "        person_data[\"Projekte\"] = projects_data\n",
    "\n",
    "        # Füge die Person zum Gesamtdictionary hinzu\n",
    "        all_persons_data.append(person_data)\n",
    "\n",
    "    else:\n",
    "        print(f\"Fehler bei der Anfrage für {url}: {response.status_code}\")\n",
    "\n",
    "# Speichere alle Daten in einer JSON-Datei\n",
    "with open('output_all_persons.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(all_persons_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Drucke eine Bestätigungsmeldung\n",
    "print(\"Alle Daten erfolgreich in 'output_all_persons.json' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
