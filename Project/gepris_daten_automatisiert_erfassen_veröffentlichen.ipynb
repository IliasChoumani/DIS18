{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "asdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste der Forscher:innen, für die die Wikidata-Einträge überprüft werden sollen\n",
    "search_terms = [\"Förstner, Konrad\", \"Becker, Anke\", \"Bork, Peer\", \"Clavel, Thomas\", \"Goesmann, Alexander\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-IDs ermitteln\n",
    "Dieses Skript durchsucht Wikidata nach bestimmten Namen, um ihre entsprechenden QIDs (Wikidata-IDs) zu finden und die zugehörigen Informationen abzurufen. Bei mehreren gefundenen Einträgen für einen Namen wird der Benutzer zur Auswahl der richtigen QID aufgefordert. Die Informationen über Personen (Menschen) werden dann in einem Dictionary gespeichert und ausgegeben.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mehrere Einträge gefunden für 'Förstner, Konrad'. Bitte wählen Sie eine QID aus:\n",
      "1: QID: Q18744528, Label: Konrad Förstner, Beschreibung: bioinformatician\n",
      "Mehrere Einträge gefunden für 'Becker, Anke'. Bitte wählen Sie eine QID aus:\n",
      "1: QID: Q21253882, Label: Anke Becker, Beschreibung: German university teacher\n",
      "2: QID: Q124022504, Label: Anke Becker, Beschreibung: German visual artist\n",
      "3: QID: Q124413921, Label: Anke Becker, Beschreibung: faculty at Harvard Business School\n",
      "Mehrere Einträge gefunden für 'Bork, Peer'. Bitte wählen Sie eine QID aus:\n",
      "1: QID: Q7160367, Label: Peer Bork, Beschreibung: German biologist and bioinformatician\n",
      "Mehrere Einträge gefunden für 'Clavel, Thomas'. Bitte wählen Sie eine QID aus:\n",
      "1: QID: Q40442760, Label: Thomas Clavel, Beschreibung: researcher\n",
      "2: QID: Q96748697, Label: Thomas Clavel, Beschreibung: French writer\n",
      "Mehrere Einträge gefunden für 'Goesmann, Alexander'. Bitte wählen Sie eine QID aus:\n",
      "1: QID: Q52422599, Label: Alexander Goesmann, Beschreibung: researcher\n",
      "Dictionary mit Informationen zu Menschen:\n",
      "Name: Konrad Förstner, Beschreibung: bioinformatician, QID: Q18744528\n",
      "Name: Anke Becker, Beschreibung: German university teacher, QID: Q21253882\n",
      "Name: Peer Bork, Beschreibung: German biologist and bioinformatician, QID: Q7160367\n",
      "Name: Thomas Clavel, Beschreibung: researcher, QID: Q40442760\n",
      "Name: Alexander Goesmann, Beschreibung: researcher, QID: Q52422599\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "# Dictionary zur Speicherung von Informationen zu Menschen\n",
    "human_info_dict = {}\n",
    "\n",
    "# Funktion zur Auswahl einer QID bei mehreren Ergebnissen für denselben Suchbegriff\n",
    "def choose_qid(qid_options, name):\n",
    "    if len(qid_options) == 1:\n",
    "        chosen_qid = qid_options[0]\n",
    "        print(f\"Nur ein Eintrag gefunden für '{name}': QID: {chosen_qid}\")\n",
    "        return chosen_qid\n",
    "    else:\n",
    "        print(f\"Mehrere Einträge gefunden für '{name}'. Bitte wählen Sie eine QID aus:\")\n",
    "        valid_entries = []\n",
    "        for idx, qid in enumerate(qid_options):\n",
    "            entity_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{qid}.json\"\n",
    "            try:\n",
    "                response = requests.get(entity_url)\n",
    "                entity_data = response.json()\n",
    "                if \"entities\" in entity_data and qid in entity_data[\"entities\"]:\n",
    "                    entity_info = entity_data[\"entities\"][qid]\n",
    "                    label = entity_info.get('labels', {}).get('en', {}).get('value', 'N/A')\n",
    "                    description = entity_info.get('descriptions', {}).get('en', {}).get('value', 'N/A')\n",
    "                    instance_of_claims = entity_info[\"claims\"].get(\"P31\", [])\n",
    "                    is_human = any(claim[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] == \"Q5\" for claim in instance_of_claims)\n",
    "                    if is_human:\n",
    "                        valid_entries.append({\n",
    "                            'qid': qid,\n",
    "                            'label': label,\n",
    "                            'description': description\n",
    "                        })\n",
    "                        print(f\"{idx + 1}: QID: {qid}, Label: {label}, Beschreibung: {description}\")\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                print(f\"Fehler bei der Abfrage für {qid}: {e}\")\n",
    "\n",
    "        while True:\n",
    "            choice = input(\"Geben Sie die Nummer der gewünschten QID ein: \")\n",
    "            try:\n",
    "                index = int(choice) - 1\n",
    "                if 0 <= index < len(valid_entries):\n",
    "                    return valid_entries[index]['qid']\n",
    "                else:\n",
    "                    print(\"Ungültige Eingabe. Bitte wählen Sie eine der angezeigten Optionen.\")\n",
    "            except ValueError:\n",
    "                print(\"Ungültige Eingabe. Bitte geben Sie eine Zahl ein.\")\n",
    "\n",
    "# Durchlauf der Suchbegriffe\n",
    "for search_term in search_terms:\n",
    "    api_url = f\"https://www.wikidata.org/w/api.php?action=query&format=json&list=search&srsearch={search_term}\"\n",
    "\n",
    "    try:\n",
    "        # HTTP-Anfrage, um die Suchergebnisse von Wikidata abzurufen\n",
    "        response = requests.get(api_url)\n",
    "        data = response.json()\n",
    "\n",
    "        # Überprüfen der Ergebnisse und Extrahieren der QIDs\n",
    "        if \"query\" in data and \"search\" in data[\"query\"]:\n",
    "            qids_options = []\n",
    "            for result in data[\"query\"][\"search\"]:\n",
    "\n",
    "                q_id = result[\"title\"]\n",
    "                qids_options.append(q_id)\n",
    "\n",
    "            # Auswahl einer QID, falls mehrere Optionen vorhanden sind\n",
    "            if len(qids_options) > 0:\n",
    "                chosen_qid = choose_qid(qids_options, search_term)\n",
    "\n",
    "                # API-Anfrage, um die JSON-Daten des Wikidata-Eintrags abzurufen\n",
    "                entity_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{chosen_qid}.json\"\n",
    "                entity_response = requests.get(entity_url)\n",
    "                entity_data = entity_response.json()\n",
    "\n",
    "                # Überprüfen, ob der Eintrag eine Instanz der Klasse \"Mensch\" (Q5) ist\n",
    "                if \"entities\" in entity_data and chosen_qid in entity_data[\"entities\"]:\n",
    "                    entity_info = entity_data[\"entities\"][chosen_qid]\n",
    "\n",
    "                    # Überprüfen, ob der Eintrag einen Menschen repräsentiert\n",
    "                    instance_of_claims = entity_info[\"claims\"].get(\"P31\", [])\n",
    "                    is_human = any(claim[\"mainsnak\"][\"datavalue\"][\"value\"][\"id\"] == \"Q5\" for claim in instance_of_claims)\n",
    "\n",
    "                    if is_human:\n",
    "                        label = entity_info.get('labels', {}).get('en', {}).get('value', 'N/A')\n",
    "                        description = entity_info.get('descriptions', {}).get('en', {}).get('value', 'N/A')\n",
    "\n",
    "                        # Speichern der Informationen im Dictionary\n",
    "                        human_info_dict[label] = {\n",
    "                            'description': description,\n",
    "                            'qid': chosen_qid\n",
    "                        }\n",
    "                    else:\n",
    "                        print(f\"Der Wikidata-Eintrag {chosen_qid} repräsentiert keine Person (Mensch).\")\n",
    "                else:\n",
    "                    print(f\"Der Wikidata-Eintrag {chosen_qid} wurde nicht gefunden oder enthält keine Daten.\")\n",
    "\n",
    "            else:\n",
    "                print(f\"Keine Ergebnisse gefunden für den Suchbegriff '{search_term}'.\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Fehler bei der HTTP-Anfrage für {search_term}: {e}\")\n",
    "    except KeyError as e:\n",
    "        print(f\"Unerwarteter JSON-Formatfehler für {search_term}: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Allgemeiner Fehler für {search_term}: {e}\")\n",
    "\n",
    "# Ausgabe des Dictionarys mit Informationen zu Menschen\n",
    "print(\"Dictionary mit Informationen zu Menschen:\")\n",
    "for name, info in human_info_dict.items():\n",
    "    print(f\"Name: {name}, Beschreibung: {info['description']}, QID: {info['qid']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Konrad Förstner': {'description': 'bioinformatician', 'qid': 'Q18744528'}, 'Anke Becker': {'description': 'German university teacher', 'qid': 'Q21253882'}, 'Peer Bork': {'description': 'German biologist and bioinformatician', 'qid': 'Q7160367'}, 'Thomas Clavel': {'description': 'researcher', 'qid': 'Q40442760'}, 'Alexander Goesmann': {'description': 'researcher', 'qid': 'Q52422599'}}\n"
     ]
    }
   ],
   "source": [
    "print(human_info_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abrufen und Speichern von GEPRIS-Daten für Wissenschaftler\n",
    "In diesem Notebook wird eine Liste von Wissenschaftlern durch die GEPRIS-Datenbank durchsucht, um ihre Projektinformationen abzurufen. Der Code verwendet requests und BeautifulSoup, um die HTML-Inhalte der Suchergebnisse zu parsen, und speichert die gefundenen Projektdaten in JSON-Dateien. Die extrahierten Informationen werden in einem speziell erstellten Ordner abgelegt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEPRIS ID Link für Förstner, Konrad: https://gepris.dfg.de/gepris/person/262159783\n",
      "GEPRIS ID Link für Becker, Anke: https://gepris.dfg.de/gepris/person/1592887\n",
      "GEPRIS ID Link für Bork, Peer: https://gepris.dfg.de/gepris/person/1011382\n",
      "GEPRIS ID Link für Clavel, Thomas: https://gepris.dfg.de/gepris/person/184220620\n",
      "GEPRIS ID Link für Goesmann, Alexander: https://gepris.dfg.de/gepris/person/188428736\n",
      "Daten für Person 'Professor Dr. Konrad  Förstner' erfolgreich in 'GeprisJsonAndCsv\\Professor-Dr-Konrad-FörstnerGepris.json' gespeichert.\n",
      "Daten für Person 'Professorin Dr. Anke  Becker' erfolgreich in 'GeprisJsonAndCsv\\Professorin-Dr-Anke-BeckerGepris.json' gespeichert.\n",
      "Daten für Person 'Professor Dr. Peer  Bork' erfolgreich in 'GeprisJsonAndCsv\\Professor-Dr-Peer-BorkGepris.json' gespeichert.\n",
      "Daten für Person 'Professor Dr. Thomas  Clavel' erfolgreich in 'GeprisJsonAndCsv\\Professor-Dr-Thomas-ClavelGepris.json' gespeichert.\n",
      "Daten für Person 'Professor Dr. Alexander  Goesmann' erfolgreich in 'GeprisJsonAndCsv\\Professor-Dr-Alexander-GoesmannGepris.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import re  # Regex-Modul zum Bereinigen des Dateinamens\n",
    "import os  # Modul zum Arbeiten mit dem Dateisystem\n",
    "\n",
    "def get_gepris_id_for_person(name):\n",
    "    search_url = \"https://gepris.dfg.de/gepris/OCTOPUS\"\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"\n",
    "    }\n",
    "    params = {\n",
    "        \"context\": \"person\",\n",
    "        \"task\": \"doSearchSimple\",\n",
    "        \"keywords_criterion\": name,\n",
    "        \"findButton\": \"historyCall\",\n",
    "        \"hitsPerPage\": 10,\n",
    "        \"index\": 0,\n",
    "        \"nurProjekteMitAB\": False\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.get(search_url, params=params, headers=headers)\n",
    "        response.raise_for_status()  # Raise an exception for bad responses\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Find the first search result link containing the name\n",
    "        search_results = soup.find_all(\"a\", href=True)\n",
    "        for result in search_results:\n",
    "            if name.lower() in result.text.lower():  # Case-insensitive comparison\n",
    "                return result['href']\n",
    "\n",
    "        print(f\"No GEPRIS ID found for {name}\")\n",
    "        return None\n",
    "    \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error accessing GEPRIS for {name}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Liste für die Ergebnisse initialisieren\n",
    "gepris_id_links = []\n",
    "\n",
    "# Durchlaufe jeden Suchbegriff und rufe die GEPRIS-ID ab\n",
    "for person_name in search_terms:\n",
    "    gepris_id_link = get_gepris_id_for_person(person_name)\n",
    "    if gepris_id_link:\n",
    "        # Vervollständige die URL mit dem Basis-Link, falls nötig\n",
    "        if not gepris_id_link.startswith(\"https://\"):\n",
    "            gepris_id_link = f\"https://gepris.dfg.de{gepris_id_link}\"\n",
    "        gepris_id_links.append(gepris_id_link)\n",
    "        print(f\"GEPRIS ID Link für {person_name}: {gepris_id_link}\")\n",
    "    else:\n",
    "        print(f\"GEPRIS ID Link für {person_name} konnte nicht gefunden werden.\")\n",
    "\n",
    "# Überprüfe, ob der Ordner 'jsonGepris' existiert, und erstelle ihn bei Bedarf\n",
    "output_folder = \"GeprisJsonAndCsv\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Durchlaufe die Liste der GEPRIS-ID-Links\n",
    "for url in gepris_id_links:\n",
    "    try:\n",
    "        # Initialisiere ein leeres Dictionary für die Person und ihre Projekte\n",
    "        person_data = {}\n",
    "\n",
    "        # Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "        response = requests.get(url)\n",
    "\n",
    "        # Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "        if response.status_code == 200:\n",
    "            # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "            # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "            person_name = soup.title.text.strip().replace(\"DFG - GEPRIS - \", \"\")\n",
    "\n",
    "            # Bereinige den Personennamen für den Dateinamen (entferne Sonderzeichen und Leerzeichen)\n",
    "            clean_person_name = re.sub(r'[^\\w\\s-]', '', person_name).strip()\n",
    "            clean_person_name = re.sub(r'[-\\s]+', '-', clean_person_name)\n",
    "\n",
    "            # Füge den bereinigten Namen zur Personendaten hinzu\n",
    "            person_data[\"Person Name\"] = person_name\n",
    "\n",
    "            # Finde alle Links auf der Seite\n",
    "            links = soup.find_all('a', href=True)\n",
    "\n",
    "            # Filtere nur die Links, die auf Projekte verweisen\n",
    "            project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "            # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "            projects_data = []\n",
    "            for project_link in project_links:\n",
    "                project_name = project_link.get_text(strip=True)\n",
    "                project_url = project_link['href']\n",
    "                projects_data.append({\n",
    "                    \"Projekt Name\": project_name,\n",
    "                    \"Projekt Link\": project_url\n",
    "                })\n",
    "\n",
    "            # Füge die Projekte zum Dictionary der Person hinzu\n",
    "            person_data[\"Projekte\"] = projects_data\n",
    "\n",
    "            # Erstelle den Dateinamen mit dem bereinigten Personennamen und füge 'Wikidata' hinzu\n",
    "            output_file = os.path.join(output_folder, f\"{clean_person_name}Gepris.json\")\n",
    "\n",
    "            # Speichere die Daten in einer JSON-Datei mit dem entsprechenden Dateinamen\n",
    "            with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "                json.dump(person_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "            print(f\"Daten für Person '{person_name}' erfolgreich in '{output_file}' gespeichert.\")\n",
    "\n",
    "        else:\n",
    "            print(f\"Fehler bei der Anfrage für {url}: {response.status_code}\")\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Fehler bei der HTTP-Anfrage für {url}: {e}\")\n",
    "    except Exception as ex:\n",
    "        print(f\"Allgemeiner Fehler für {url}: {ex}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Informationen in einer Liste zusammengefasst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alle Daten erfolgreich in 'GeprisJsonAndCsv\\output_all_persons_names_projects_links.json' gespeichert.\n"
     ]
    }
   ],
   "source": [
    "# Initialisiere eine leere Liste für alle Personen und ihre Projekte\n",
    "all_persons_data = []\n",
    "\n",
    "# Durchlaufe die Liste aller Personen\n",
    "for url in gepris_id_links:\n",
    "    # Initialisiere ein leeres Dictionary für die Person und ihre Projekte\n",
    "    person_data = {}\n",
    "\n",
    "    # Führe eine HTTP-Anfrage durch, um den HTML-Inhalt der Seite zu erhalten\n",
    "    response = requests.get(url)\n",
    "\n",
    "    # Überprüfe, ob die Anfrage erfolgreich war (Status-Code 200)\n",
    "    if response.status_code == 200:\n",
    "        # Parsen des HTML-Inhalts mit Beautiful Soup\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Extrahiere den Namen der Person (aus dem Titel der Seite)\n",
    "        person_name = soup.title.text.strip().replace(\"DFG - GEPRIS - \", \"\")\n",
    "        person_data[\"Person Name\"] = person_name\n",
    "\n",
    "        # Finde alle Links auf der Seite\n",
    "        links = soup.find_all('a', href=True)\n",
    "\n",
    "        # Filtere nur die Links, die auf Projekte verweisen\n",
    "        project_links = [link for link in links if '/gepris/projekt/' in link['href']]\n",
    "\n",
    "        # Extrahiere den Text (Namen) und den Link aus den gefundenen Projekt-Links\n",
    "        projects_data = []\n",
    "        for project_link in project_links:\n",
    "            project_name = project_link.get_text(strip=True)\n",
    "            project_url = project_link['href']\n",
    "            projects_data.append({\n",
    "                \"Projekt Name\": project_name,\n",
    "                \"Projekt Link\": project_url\n",
    "            })\n",
    "\n",
    "        # Füge die Projekte zum Dictionary der Person hinzu\n",
    "        person_data[\"Projekte\"] = projects_data\n",
    "\n",
    "        # Füge die Person zum Gesamtdictionary hinzu\n",
    "        all_persons_data.append(person_data)\n",
    "\n",
    "    else:\n",
    "        print(f\"Fehler bei der Anfrage für {url}: {response.status_code}\")\n",
    "\n",
    "# Überprüfe, ob der Ordner 'jsonGepris' existiert, und erstelle ihn bei Bedarf\n",
    "output_folder = \"GeprisJsonAndCsv\"\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# Speichere alle Daten in einer JSON-Datei im Unterordner 'jsonGepris'\n",
    "output_file = os.path.join(output_folder, 'output_all_persons_names_projects_links.json')\n",
    "with open(output_file, 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(all_persons_data, json_file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Drucke eine Bestätigungsmeldung\n",
    "print(f\"Alle Daten erfolgreich in '{output_file}' gespeichert.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prüfen ob Projekte bereits auf wikidata angelegt sind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Gepris Projekte der zu überprüfenden Forscher in einer Liste speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Liste der Projekt-IDs:\n",
      "['433110396', '492813820', '509313233', '460129525', '521476232', '520751609', '5095944', '5310710', '71821268', '40014398', '198305071', '218318381', '314772579', '431352836', '230488449', '505997786', '5365365', '224619622', '326552732', '460129525', '18572122', '5131248', '290363600', '24122740', '441914366', '460129525', '5413368', '453182863', '453229399', '513892404', '242504939', '316102599', '418004173', '445552570', '516780480', '418598556', '418603037', '424650015', '424795268', '460129525', '447603908', '456668568', '458957343', '319835486', '325443116', '221270173', '183605059', '284237345', '442032008', '460129525', '255821879', '507302435', '270041755', '491261247']\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import requests\n",
    "\n",
    "# Pfad zur JSON-Datei (relativ zum aktuellen Verzeichnis)\n",
    "json_file_path = \"GeprisJsonAndCsv/output_all_persons_names_projects_links.json\"\n",
    "\n",
    "# Menge zur Speicherung der Projekt-IDs\n",
    "project_ids_list = []\n",
    "\n",
    "# Vollständigen Pfad zur JSON-Datei erstellen\n",
    "full_json_file_path = os.path.join(os.getcwd(), json_file_path)\n",
    "\n",
    "# JSON-Datei einlesen und Daten laden\n",
    "with open(full_json_file_path, 'r', encoding='utf-8') as json_file:\n",
    "    data = json.load(json_file)\n",
    "\n",
    "# Durchlaufe jeden Eintrag in der Liste\n",
    "for entry in data:\n",
    "    # Extrahiere die Projekte aus dem Eintrag\n",
    "    projects = entry.get('Projekte', [])\n",
    "    \n",
    "    # Durchlaufe die Projekte und extrahiere die Projekt-IDs\n",
    "    for project in projects:\n",
    "        project_link = project.get('Projekt Link', '')\n",
    "        \n",
    "        # Extrahiere die Projekt-ID aus dem Projekt-Link\n",
    "        if project_link:\n",
    "            project_id = project_link.split('/')[-1]\n",
    "            # Füge die Projekt-ID zur Liste hinzu (Vermeidung von Duplikaten durch Set)\n",
    "            project_ids_list.append(project_id)\n",
    "\n",
    "# Ausgabe der gesammelten Projekt-IDs\n",
    "print(\"Liste der Projekt-IDs:\")\n",
    "print(project_ids_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abgleich Liste und wikidata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefundene Einträge:\n",
      "Projekt-ID: 460129525, Q-ID: Q99534506, Label: NFDI4Microbiota\n",
      "Projekt-ID: 5095944, Q-ID: Q116062070, Label: \n",
      "Projekt-ID: 71821268, Q-ID: Q116128332, Label: \n",
      "Projekt-ID: 40014398, Q-ID: Q116121999, Label: \n",
      "Projekt-ID: 5365365, Q-ID: Q116092079, Label: \n",
      "Projekt-ID: 460129525, Q-ID: Q99534506, Label: NFDI4Microbiota\n",
      "Projekt-ID: 18572122, Q-ID: Q116114558, Label: \n",
      "Projekt-ID: 5131248, Q-ID: Q116063769, Label: \n",
      "Projekt-ID: 441914366, Q-ID: Q98380337, Label: GHGA\n",
      "Projekt-ID: 460129525, Q-ID: Q99534506, Label: NFDI4Microbiota\n",
      "Projekt-ID: 460129525, Q-ID: Q99534506, Label: NFDI4Microbiota\n",
      "Projekt-ID: 442032008, Q-ID: Q98380341, Label: NFDI4Biodiversity\n",
      "Projekt-ID: 460129525, Q-ID: Q99534506, Label: NFDI4Microbiota\n"
     ]
    }
   ],
   "source": [
    "found_entries = []\n",
    "\n",
    "for project in project_ids_list:\n",
    "    api_url = f\"https://www.wikidata.org/w/api.php?action=query&format=json&list=search&srsearch=P4870+{project}\"\n",
    "\n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "\n",
    "    if \"query\" in data and \"search\" in data[\"query\"]:\n",
    "        for result in data[\"query\"][\"search\"]:\n",
    "            q_id = result[\"title\"]\n",
    "\n",
    "            # Weitere Abfrage, um das Label des Wikidata-Eintrags abzurufen\n",
    "            entity_url = f\"https://www.wikidata.org/wiki/Special:EntityData/{q_id}.json\"\n",
    "            entity_response = requests.get(entity_url)\n",
    "            entity_data = entity_response.json()\n",
    "\n",
    "            if \"entities\" in entity_data and q_id in entity_data[\"entities\"]:\n",
    "                entity_info = entity_data[\"entities\"][q_id]\n",
    "                label = entity_info.get('labels', {}).get('en', {}).get('value', '')  # Das Label des Eintrags\n",
    "\n",
    "                # Füge das gefundene Ergebnis der Liste hinzu\n",
    "                found_entries.append({\n",
    "                    \"project_id\": project,\n",
    "                    \"q_id\": q_id,\n",
    "                    \"label\": label\n",
    "                })\n",
    "\n",
    "# Ausgabe der gefundenen Einträge\n",
    "print(\"Gefundene Einträge:\")\n",
    "for entry in found_entries:\n",
    "    print(f\"Projekt-ID: {entry['project_id']}, Q-ID: {entry['q_id']}, Label: {entry['label']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alle Projekte ohne Eintrag in einer Liste speichern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gefilterte Projekt-IDs:\n",
      "['433110396', '492813820', '509313233', '521476232', '520751609', '5310710', '198305071', '218318381', '314772579', '431352836', '230488449', '505997786', '224619622', '326552732', '290363600', '24122740', '5413368', '453182863', '453229399', '513892404', '242504939', '316102599', '418004173', '445552570', '516780480', '418598556', '418603037', '424650015', '424795268', '447603908', '456668568', '458957343', '319835486', '325443116', '221270173', '183605059', '284237345', '255821879', '507302435', '270041755', '491261247']\n"
     ]
    }
   ],
   "source": [
    "# Extrahiere die Projekt-IDs aus den gefundenen Einträgen\n",
    "found_project_ids = {entry['project_id'] for entry in found_entries}\n",
    "\n",
    "# Erstelle eine neue Liste, die nur die Projekt-IDs enthält, die nicht in den gefundenen Einträgen sind\n",
    "non_existing_entries = [project_id for project_id in project_ids_list if project_id not in found_project_ids]\n",
    "\n",
    "# Ausgabe der gefilterten Liste\n",
    "print(\"Gefilterte Projekt-IDs:\")\n",
    "print(non_existing_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## json zum Upload vorbereiten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json mit gepris-Projekten reinigen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten wurden erfolgreich bearbeitet und in \"GeprisJsonAndCsv/output_all_persons_names_projects_cleaned.json\" gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import re\n",
    "\n",
    "input_file = 'GeprisJsonAndCsv/output_all_persons_names_projects_links.json'\n",
    "output_file = 'GeprisJsonAndCsv/output_all_persons_names_projects_cleaned.json'\n",
    "\n",
    "# Funktion zum Entfernen der Titel, Punkte und doppelten Leerzeichen\n",
    "def clean_person_name(name):\n",
    "    # Entferne Titel\n",
    "    name = re.sub(r'\\b(Professor|Professorin|Dr\\.?)\\b', '', name, flags=re.IGNORECASE)\n",
    "    # Entferne überflüssige Punkte\n",
    "    name = re.sub(r'\\.', '', name)\n",
    "    # Entferne doppelte Leerzeichen und überflüssige Leerzeichen am Anfang und Ende\n",
    "    name = re.sub(r'\\s+', ' ', name).strip()\n",
    "    return name\n",
    "\n",
    "# Funktion zum Entfernen des spezifischen Parts aus dem Projekt Link\n",
    "def clean_project_link(link):\n",
    "    return re.sub(r'^/gepris/projekt/', '', link)\n",
    "\n",
    "# Lade die JSON-Daten aus einer Datei\n",
    "def load_json(filename):\n",
    "    try:\n",
    "        with open(filename, 'r', encoding='utf-8') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Die Datei {filename} wurde nicht gefunden.\")\n",
    "        raise\n",
    "    except json.JSONDecodeError:\n",
    "        print(f\"Fehler beim Dekodieren der JSON-Datei {filename}.\")\n",
    "        raise\n",
    "\n",
    "# Speichere die bearbeiteten JSON-Daten in eine Datei\n",
    "def save_json(data, filename):\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as file:\n",
    "            json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "    except IOError:\n",
    "        print(f\"Fehler beim Schreiben der Datei {filename}.\")\n",
    "        raise\n",
    "\n",
    "# Hauptprogramm\n",
    "def main(input_file, output_file):\n",
    "    # JSON-Daten laden\n",
    "    data = load_json(input_file)\n",
    "    \n",
    "    # Namen bearbeiten und Projekt Links reinigen\n",
    "    for person in data:\n",
    "        person['Person Name'] = clean_person_name(person['Person Name'])\n",
    "        for projekt in person.get('Projekte', []):\n",
    "            projekt['Projekt Link'] = clean_project_link(projekt['Projekt Link'])\n",
    "    \n",
    "    # Bearbeitete JSON-Daten speichern\n",
    "    save_json(data, output_file)\n",
    "    print(f'Daten wurden erfolgreich bearbeitet und in \"{output_file}\" gespeichert.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Struktur der json anpassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten wurden erfolgreich bearbeitet und in \"GeprisJsonAndCsv/output_restructured_projects.json\" gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'GeprisJsonAndCsv/output_all_persons_names_projects_cleaned.json'\n",
    "output_file = 'GeprisJsonAndCsv/output_restructured_projects.json'\n",
    "\n",
    "# Lade die JSON-Daten aus einer Datei\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Speichere die bearbeiteten JSON-Daten in eine Datei\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Hauptprogramm zur Umstrukturierung der JSON-Daten\n",
    "def restructure_json(data):\n",
    "    # Schritt 1: Erstelle ein Mapping von Projekt-IDs zu Namen und Teilnehmern\n",
    "    project_participants = {}\n",
    "    \n",
    "    for person in data:\n",
    "        participant_id = person['Person Name']\n",
    "        for projekt in person.get('Projekte', []):\n",
    "            project_name = projekt['Projekt Name']\n",
    "            project_link = projekt['Projekt Link']\n",
    "            \n",
    "            if project_link not in project_participants:\n",
    "                project_participants[project_link] = {\n",
    "                    \"name\": project_name,\n",
    "                    \"gepris_id\": project_link,\n",
    "                    \"participants\": []\n",
    "                }\n",
    "            if participant_id not in project_participants[project_link][\"participants\"]:\n",
    "                project_participants[project_link][\"participants\"].append(participant_id)\n",
    "    \n",
    "    # Schritt 2: Konvertiere das Mapping in eine Liste von Projekten\n",
    "    projects = list(project_participants.values())\n",
    "    \n",
    "    return projects\n",
    "\n",
    "# Hauptfunktion\n",
    "def main(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    structured_data = restructure_json(data)\n",
    "    save_json(structured_data, output_file)\n",
    "    print(f'Daten wurden erfolgreich bearbeitet und in \"{output_file}\" gespeichert.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json mit gepris_infos mit den Q-IDs der Forschenden verknüpfen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten wurden erfolgreich bearbeitet und in \"GeprisJsonAndCsv/output_restructured_projects_with_ids.json\" gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "input_file = 'GeprisJsonAndCsv/output_restructured_projects.json'\n",
    "output_file = 'GeprisJsonAndCsv/output_restructured_projects_with_ids.json'\n",
    "\n",
    "# Dictionary mit weiteren Informationen zu den Forschern\n",
    "human_info_dict = {\n",
    "    'Konrad Förstner': {'description': 'bioinformatician', 'qid': 'Q18744528'},\n",
    "    'Anke Becker': {'description': 'German university teacher', 'qid': 'Q21253882'},\n",
    "    'Peer Bork': {'description': 'German biologist and bioinformatician', 'qid': 'Q7160367'},\n",
    "    'Thomas Clavel': {'description': 'researcher', 'qid': 'Q40442760'},\n",
    "    'Alexander Goesmann': {'description': 'researcher', 'qid': 'Q52422599'}\n",
    "}\n",
    "\n",
    "# Lade die JSON-Daten aus einer Datei\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Speichere die bearbeiteten JSON-Daten in eine Datei\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Funktion zur Umstrukturierung der JSON-Daten\n",
    "def restructure_json(data):\n",
    "    projects_dict = {}\n",
    "\n",
    "    for project in data:\n",
    "        project_name = project['name']\n",
    "        project_id = project['gepris_id']\n",
    "        participants = project['participants']\n",
    "        \n",
    "        if project_id not in projects_dict:\n",
    "            projects_dict[project_id] = {\n",
    "                \"name\": project_name,\n",
    "                \"gepris_id\": project_id,\n",
    "                \"participants\": set(participants)\n",
    "            }\n",
    "        else:\n",
    "            projects_dict[project_id][\"participants\"].update(participants)\n",
    "    \n",
    "    # Konvertiere das Dictionary in eine Liste von Projekten\n",
    "    projects = []\n",
    "    for proj_info in projects_dict.values():\n",
    "        # Erstelle die Teilnehmer-ID-Liste basierend auf human_info_dict\n",
    "        participants_ids = [human_info_dict.get(p, {}).get('qid', '') for p in proj_info[\"participants\"]]\n",
    "        \n",
    "        projects.append({\n",
    "            \"name\": proj_info[\"name\"],\n",
    "            \"gepris_id\": proj_info[\"gepris_id\"],\n",
    "            \"participants_id\": participants_ids\n",
    "        })\n",
    "    \n",
    "    return projects\n",
    "\n",
    "# Hauptfunktion\n",
    "def main(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    structured_data = restructure_json(data)\n",
    "    save_json(structured_data, output_file)\n",
    "    print(f'Daten wurden erfolgreich bearbeitet und in \"{output_file}\" gespeichert.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "json mit nur Projekten, die angelegt werden müssen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Daten wurden erfolgreich gefiltert und in \"GeprisJsonAndCsv/output_restructured_projects_filtered.json\" gespeichert.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Datei- und Liste-Definitionen\n",
    "input_file = 'GeprisJsonAndCsv/output_restructured_projects.json'\n",
    "output_file = 'GeprisJsonAndCsv/output_restructured_projects_filtered.json'\n",
    "\n",
    "# Liste der gepris_ids, die beibehalten werden sollen\n",
    "non_existing_entries = [\n",
    "    '433110396', '492813820', '509313233', '521476232', '520751609', '5310710',\n",
    "    '198305071', '218318381', '314772579', '431352836', '230488449', '505997786',\n",
    "    '224619622', '326552732', '290363600', '24122740', '5413368', '453182863',\n",
    "    '453229399', '513892404', '242504939', '316102599', '418004173', '445552570',\n",
    "    '516780480', '418598556', '418603037', '424650015', '424795268', '447603908',\n",
    "    '456668568', '458957343', '319835486', '325443116', '221270173', '183605059',\n",
    "    '284237345', '255821879', '507302435', '270041755', '491261247'\n",
    "]\n",
    "\n",
    "# Dictionary mit weiteren Informationen zu den Forschern\n",
    "human_info_dict = {\n",
    "    'Konrad Förstner': {'description': 'bioinformatician', 'qid': 'Q18744528'},\n",
    "    'Anke Becker': {'description': 'German university teacher', 'qid': 'Q21253882'},\n",
    "    'Peer Bork': {'description': 'German biologist and bioinformatician', 'qid': 'Q7160367'},\n",
    "    'Thomas Clavel': {'description': 'researcher', 'qid': 'Q40442760'},\n",
    "    'Alexander Goesmann': {'description': 'researcher', 'qid': 'Q52422599'}\n",
    "}\n",
    "\n",
    "# Lade die JSON-Daten aus einer Datei\n",
    "def load_json(filename):\n",
    "    with open(filename, 'r', encoding='utf-8') as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Speichere die bearbeiteten JSON-Daten in eine Datei\n",
    "def save_json(data, filename):\n",
    "    with open(filename, 'w', encoding='utf-8') as file:\n",
    "        json.dump(data, file, ensure_ascii=False, indent=2)\n",
    "\n",
    "# Funktion zur Umstrukturierung der JSON-Daten\n",
    "def filter_projects(data):\n",
    "    filtered_projects = []\n",
    "\n",
    "    for project in data:\n",
    "        project_id = project.get('gepris_id')\n",
    "        if project_id in non_existing_entries:\n",
    "            # Erstelle die Teilnehmer-ID-Liste basierend auf human_info_dict\n",
    "            participants_ids = [\n",
    "                human_info_dict.get(p, {}).get('qid', '')\n",
    "                for p in project.get('participants', [])\n",
    "                if human_info_dict.get(p) is not None\n",
    "            ]\n",
    "            \n",
    "            filtered_projects.append({\n",
    "                \"name\": project.get('name'),\n",
    "                \"gepris_id\": project_id,\n",
    "                \"participants_id\": participants_ids\n",
    "            })\n",
    "\n",
    "    return filtered_projects\n",
    "\n",
    "# Hauptfunktion\n",
    "def main(input_file, output_file):\n",
    "    data = load_json(input_file)\n",
    "    filtered_data = filter_projects(data)\n",
    "    save_json(filtered_data, output_file)\n",
    "    print(f'Daten wurden erfolgreich gefiltert und in \"{output_file}\" gespeichert.')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(input_file, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Anmeldung bei Wikidata...\n",
      "DEBUG:urllib3.connectionpool:Starting new HTTPS connection (1): www.wikidata.org:443\n",
      "DEBUG:urllib3.connectionpool:https://www.wikidata.org:443 \"GET /w/api.php?action=query&meta=tokens&type=login&format=json HTTP/1.1\" 200 110\n",
      "DEBUG:urllib3.connectionpool:https://www.wikidata.org:443 \"POST /w/api.php HTTP/1.1\" 200 95\n",
      "DEBUG:urllib3.connectionpool:https://www.wikidata.org:443 \"GET /w/api.php?action=query&meta=tokens&format=json HTTP/1.1\" 200 109\n",
      "INFO:root:Erfolgreich bei Wikidata angemeldet.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'labels': {'de': 'sRNARegNet - Vergleichende Analyse der regulatorischen Netzwerke kleiner RNA in Gammaproteobacteria'}, 'claims': {'P31': ['Q1298668'], 'P123': ['433110396'], 'P50': ['Q18744528']}}, {'labels': {'de': 'PIXLS - Preprint Information eXtraction for Life Sciences'}, 'claims': {'P31': ['Q1298668'], 'P123': ['492813820'], 'P50': ['Q18744528']}}, {'labels': {'de': 'Automatic Quality Assessment: NLP-Verfahren zur semantischen Kartierung von Lebenswissenschaftlichen Texten (AQUAS)'}, 'claims': {'P31': ['Q1298668'], 'P123': ['509313233'], 'P50': ['Q18744528']}}, {'labels': {'de': 'NFDI4Microbiota - Nationale Forschungsdateninfrastruktur für Mikrobiota-Forschung'}, 'claims': {'P31': ['Q1298668'], 'P123': ['460129525'], 'P50': ['Q52422599', 'Q18744528', 'Q7160367', 'Q40442760', 'Q21253882']}}, {'labels': {'de': 'Base4NFDI - Basisdienste für die NFDI (NFDI4Microbiota)'}, 'claims': {'P31': ['Q1298668'], 'P123': ['521476232'], 'P50': ['Q18744528']}}, {'labels': {'de': 'SP4 - Wechselspiel zwischen Umweltbedingungen, Baumwachstum und eichen-assoziierter Mikrobiome im Boden, der Endo- und Phyllosphäre'}, 'claims': {'P31': ['Q1298668'], 'P123': ['520751609'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Polymerisation und Export der sauren Exopolysaccharide Succinoglycan und Galactoglucan von Sinorhizobium meliloti'}, 'claims': {'P31': ['Q1298668'], 'P123': ['5095944'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Genetik'}, 'claims': {'P31': ['Q1298668'], 'P123': ['5310710'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Regulation of the metabolic adaptation of the phytopathogenic bacteria Xanthomonas campestris pv. campestris and Xanthomonas campestris pv. armoraciae during infection of Arabidopsis thaliana'}, 'claims': {'P31': ['Q1298668'], 'P123': ['71821268'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Target identification and functional analysis of regulatory small RNAs in Sinorhizobium meliloti and related alpha-proteobacteria'}, 'claims': {'P31': ['Q1298668'], 'P123': ['40014398'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Molecular principles of c-di-GMP signaling cascades in Escherichia coli'}, 'claims': {'P31': ['Q1298668'], 'P123': ['198305071'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Molekulare Mechanismen und Vorteile phänotypischer Heterogenität in Sinorhizobium meliloti Populationen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['218318381'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Vielfalt und funktionelle Diversität der Signalwege zyklischer Mononukleotide in Sinorhizobium meliloti'}, 'claims': {'P31': ['Q1298668'], 'P123': ['314772579'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Echtzeitanalyse des rhizobiellen Lebensstilwechsels von der Rhizosphäre in den Wirtspflanzen-geprägten Kolonisierungsmodus'}, 'claims': {'P31': ['Q1298668'], 'P123': ['431352836'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Laborautomationsplattform'}, 'claims': {'P31': ['Q1298668'], 'P123': ['230488449'], 'P50': ['Q21253882']}}, {'labels': {'de': 'GRK 2937:\\xa0Nukleotid Metabolismus in Mikroben'}, 'claims': {'P31': ['Q1298668'], 'P123': ['505997786'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Kräfte und molekulare Mechanismen der DNA-Protein-Bindung'}, 'claims': {'P31': ['Q1298668'], 'P123': ['5365365'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Gegenläufige Regulation der Oberflächenpolysaccharid-Biosynthese und Motilität in Sinorhizobium meliloti'}, 'claims': {'P31': ['Q1298668'], 'P123': ['224619622'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Räumliche Organisation und zeitliche Dynamik alpha-rhizobieller Multi-Replikon-Genome'}, 'claims': {'P31': ['Q1298668'], 'P123': ['326552732'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Anpassung von Synechococcus elongatus PCC 7942 an metabolische Stressbedingungen: Analyse des molekularbiologischen Systems eines obligat oxygen-photoautotrophen Prokaryonten'}, 'claims': {'P31': ['Q1298668'], 'P123': ['18572122'], 'P50': ['Q21253882']}}, {'labels': {'de': 'Stoffwechselwegvorhersagen mittels Genomanalyse und Enzymkettenmodellierung'}, 'claims': {'P31': ['Q1298668'], 'P123': ['5131248'], 'P50': ['Q7160367']}}, {'labels': {'de': 'Molekulare Pathogenese und Immunbiologie intestinaler MSI Tumoren in einem DNA Mismatch Reparatur-defizienten Mausmodell'}, 'claims': {'P31': ['Q1298668'], 'P123': ['290363600'], 'P50': ['Q7160367']}}, {'labels': {'de': 'EXC 81:\\xa0Zelluläre Netzwerke: Von der Analyse molekularer Mechanismen zum quantitativen Verständnis komplexer Funktionen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['24122740'], 'P50': ['Q7160367']}}, {'labels': {'de': 'GHGA – Deutsches Humangenom-Phenomarchiv'}, 'claims': {'P31': ['Q1298668'], 'P123': ['441914366'], 'P50': ['Q7160367']}}, {'labels': {'de': 'Molekularbiologie'}, 'claims': {'P31': ['Q1298668'], 'P123': ['5413368'], 'P50': ['Q7160367']}}, {'labels': {'de': 'Ökologie und Funktionen kleiner SCIFF-Proteine im Darmmikrobiom'}, 'claims': {'P31': ['Q1298668'], 'P123': ['453182863'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Kausale Rolle des Gallensäuremetabolismus durch das Mikrobiom bei Dickdarmkrebs'}, 'claims': {'P31': ['Q1298668'], 'P123': ['453229399'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Personalisierte Wirkung von Antibiotika auf das Darmmikrobiom mittels Hoch-Durchsatz Kultivierung'}, 'claims': {'P31': ['Q1298668'], 'P123': ['513892404'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Funktionen und metabolische Adaptation von dominanten Darmbakterien der Familie Coriobacteriaceae im Kontext des Lipidmetabolismus des Wirtes'}, 'claims': {'P31': ['Q1298668'], 'P123': ['242504939'], 'P50': ['Q40442760']}}, {'labels': {'de': '- MIMIC -\\nKomplementäre Nutzung von innovativen kulturbasierten und molekularen Methoden zur Herstellung von Minimalen Mikrobiellen Konsortien'}, 'claims': {'P31': ['Q1298668'], 'P123': ['316102599'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Funktionelle Analyse der Mikrobiom-Wirt Interaktion während des postnatalen „window of opportunity“ mithilfe eines mehrdimensionalen Sequenzieransatzes'}, 'claims': {'P31': ['Q1298668'], 'P123': ['418004173'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Plattform zur automatisierten Isolierung und Charakterisierung von anaeroben Bakterien'}, 'claims': {'P31': ['Q1298668'], 'P123': ['445552570'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Perinatale Interaktionen zwischen Mikrobiom und intestinalen Makrophagen bei der Kontrolle mukosaler Pathogene'}, 'claims': {'P31': ['Q1298668'], 'P123': ['516780480'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Gezieltes Design und Manipulation bakterieller Minimalkonsortien zum gezielten Austausch in Mikrobiomen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['418598556'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Gnotobiotische Mausmodelle und minimale bakterielle Konsortien'}, 'claims': {'P31': ['Q1298668'], 'P123': ['418603037'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Effekte des mikrobiellen Lipidstoffwechsels auf die Darm-Leber Achse'}, 'claims': {'P31': ['Q1298668'], 'P123': ['424650015'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Funktionelle Mikrobiom-Analyse'}, 'claims': {'P31': ['Q1298668'], 'P123': ['424795268'], 'P50': ['Q40442760']}}, {'labels': {'de': 'Bioinformatische Methoden zur Analyse der mutualistischen RNA-Kommunikation'}, 'claims': {'P31': ['Q1298668'], 'P123': ['447603908'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Bioinformatische Workflows zur skalierbaren Analyse von pflanzlichen “Omics”-Daten in Cloud-Computing-Umgebungen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['456668568'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Evolution von Gennetzwerken: Die Ranunculales als Modellordnung für evolutionäre Innovationen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['458957343'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Core Unit - Genom Signaturen und integrierte Systembiologie der Pathogen-Wirts Interaktion'}, 'claims': {'P31': ['Q1298668'], 'P123': ['319835486'], 'P50': ['Q52422599']}}, {'labels': {'de': 'GRK 2355:\\xa0Regulatorische Netzwerke im mRNA-Lebenszyklus:\\nvon kodierenden zu nichtkodierenden RNAs'}, 'claims': {'P31': ['Q1298668'], 'P123': ['325443116'], 'P50': ['Q52422599']}}, {'labels': {'de': 'GRK 1906:\\xa0Informatische Methoden für die Analyse von Genomdiversität und -dynamik'}, 'claims': {'P31': ['Q1298668'], 'P123': ['221270173'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Bioinformatische Analyse genomweiter Datensätze'}, 'claims': {'P31': ['Q1298668'], 'P123': ['183605059'], 'P50': ['Q52422599']}}, {'labels': {'de': 'KFO 309:\\xa0Virus-induced Lung Injury:\\nPathobiology and Novel Therapeutic Strategies'}, 'claims': {'P31': ['Q1298668'], 'P123': ['284237345'], 'P50': ['Q52422599']}}, {'labels': {'de': 'NFDI4Biodiversität – Nationale Forschungsdateninfrastruktur für Biodiversitäts-, Ökologie- und Umweltdaten'}, 'claims': {'P31': ['Q1298668'], 'P123': ['442032008'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Genom-weite Untersuchungen zur Artbildung unter sympatrischen Bedingungen in einem marinen Säuger: demographische Entwicklungsgeschichte, Populationsstruktur und lokale Anpassung im vom Aussterben bedrohten Galapagos Seelöwen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['255821879'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Die Rolle von SOCS1 Mutationen bei der Pathogenese von Hodgkin Lymphomen und Diffus großzelligen B-Zell Lymphomen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['507302435'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Nukleosom-Präservierung in Säugetier-Spermien: ein epigenetisches Programm zur Wahrung der gesunden männlichen Reproduktion'}, 'claims': {'P31': ['Q1298668'], 'P123': ['270041755'], 'P50': ['Q52422599']}}, {'labels': {'de': 'Untersuchung der Mechanismen für die Stabilität von mcr-1-kodierenden IncX4-Mechanismen'}, 'claims': {'P31': ['Q1298668'], 'P123': ['491261247'], 'P50': ['Q52422599']}}]\n"
     ]
    }
   ],
   "source": [
    "from wikidataintegrator import wdi_core  # wdi_edit, wdi_login,\n",
    "import json\n",
    "import logging\n",
    "\n",
    "# Konfiguration des Loggings\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "# Deine Wikidata-Anmeldedaten\n",
    "WDUSER = 'Lennart237THKoeln@AWDE-Bot'\n",
    "WDPASS = 's1vrjdampl8djincgnk5ic16s7gf8khq'\n",
    "\n",
    "# Anmelden\n",
    "logging.info(\"Anmeldung bei Wikidata...\")\n",
    "try:\n",
    "    login_instance = wdi_login.WDLogin(user=WDUSER, pwd=WDPASS)\n",
    "    logging.info(\"Erfolgreich bei Wikidata angemeldet.\")\n",
    "except Exception as e:\n",
    "    logging.error(f\"Fehler bei der Anmeldung: {e}\")\n",
    "    exit()\n",
    "\n",
    "# Lese die JSON-Datei\n",
    "with open('GeprisJsonAndCsv/output_restructured_projects_with_ids.json', 'r') as file:\n",
    "    projects = json.load(file)\n",
    "\n",
    "# Erstelle eine Liste von Items, die hochgeladen werden sollen\n",
    "items_to_upload = []\n",
    "\n",
    "for project in projects:\n",
    "    # Erstelle ein neues Wikidata-Item\n",
    "    item_data = {\n",
    "        'labels': {\n",
    "            'de': project[\"name\"],\n",
    "        },\n",
    "        'claims': {\n",
    "            'P31': ['Q1298668']  # Setze den Typ des Projekts\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Füge die GEPRIS ID als Statement hinzu (hier als Beispiel, dies hängt von den tatsächlichen Anforderungen ab)\n",
    "    item_data['claims']['P4870'] = [project[\"gepris_id\"]]\n",
    "\n",
    "    # Füge alle Teilnehmer IDs als Statements hinzu\n",
    "    for participant_id in project[\"participants_id\"]:\n",
    "        item_data.setdefault('claims', {}).setdefault('P710', []).append(participant_id)\n",
    "\n",
    "    items_to_upload.append(item_data)\n",
    "\n",
    "print(items_to_upload)\n",
    "\n",
    "# # Hochladen der Items\n",
    "# for item_data in items_to_upload:\n",
    "#     try:\n",
    "#         item = wdi_core.WDItemEngine(data=item_data, login=login_instance)\n",
    "#         item.write()\n",
    "#         logging.info(f\"Projekt '{item_data['labels']['de']}' wurde erfolgreich erstellt.\")\n",
    "#     except Exception as e:\n",
    "#         logging.error(f\"Fehler beim Erstellen des Projekts '{item_data['labels']['de']}': {e}\")\n",
    "\n",
    "# logging.info(\"Alle Projekte wurden erfolgreich erstellt.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
